# @package _group_

scheduler:
  _target_: torch.optim.lr_scheduler.OneCycleLR
  max_lr: ${optimizer.lr}
  epochs: ${trainer.max_epochs}
  pct_start: 0.2
  steps_per_epoch: -1
  # need to set to number because of tensorboard logger

pytorch_lightning_params:
  interval: step
